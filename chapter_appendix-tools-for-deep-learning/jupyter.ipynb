{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "NL-XITHWRfPS"
      },
      "id": "NL-XITHWRfPS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the File"
      ],
      "metadata": {
        "id": "-LOK_TkmepEh"
      },
      "id": "-LOK_TkmepEh"
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(f\"total no' of characters in the file are : {len(raw_text)}\")\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmH_fUEDRgNA",
        "outputId": "3db84b2d-a630-443b-87c8-e0c7d84c8070"
      },
      "id": "TmH_fUEDRgNA",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total no' of characters in the file are : 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using re lib to split the text data\n"
      ],
      "metadata": {
        "id": "Ien61EBdYsOo"
      },
      "id": "Ien61EBdYsOo"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, I am learning, and this is test string -- !\"\n",
        "# result  = re.split(r'\\s',text)\n",
        "# result = re.split(r'([,.]|\\s)',text)\n",
        "# print(result)\n",
        "# simply in bilow 2 line we are doing tokenization\n",
        "result = re.split(r'([,.:;\"?_!()]|--|\\s)',text)\n",
        "print(result)\n",
        "result =  [item for item in result if item.strip()] # item.strip returns false when there is space\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcAonS4xYzx_",
        "outputId": "ac21e0bb-50d1-4b61-8dca-972c92eff8ff"
      },
      "id": "kcAonS4xYzx_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'I', ' ', 'am', ' ', 'learning', ',', '', ' ', 'and', ' ', 'this', ' ', 'is', ' ', 'test', ' ', 'string', ' ', '--', ' ', '!']\n",
            "['Hello', ',', '', ' ', 'I', ' ', 'am', ' ', 'learning', ',', '', ' ', 'and', ' ', 'this', ' ', 'is', ' ', 'test', ' ', 'string', ' ', '', '--', '', ' ', '', '!', '']\n",
            "['Hello', ',', 'I', 'am', 'learning', ',', 'and', 'this', 'is', 'test', 'string', '--', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now performing tokenization on our raw_text"
      ],
      "metadata": {
        "id": "Ux9kHivRcYW9"
      },
      "id": "Ux9kHivRcYW9"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;\"?_!()]|--|\\s)',raw_text)\n",
        "preprocessed =  [item for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))\n",
        "print(preprocessed[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EUVvr-ccdyD",
        "outputId": "d1a062d5-c94d-446e-9674-61d8a2fdb7ad"
      },
      "id": "0EUVvr-ccdyD",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4519\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "rXa4qu5hezNs"
      },
      "id": "rXa4qu5hezNs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting token into token ID"
      ],
      "metadata": {
        "id": "hVz1ayIme_Id"
      },
      "id": "hVz1ayIme_Id"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(preprocessed)}\n",
        "print(len(vocab))\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i>=50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhzNcfzUe_9l",
        "outputId": "cb1c1227-8bb7-432f-e791-8a0762f63260"
      },
      "id": "MhzNcfzUe_9l",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1154\n",
            "('I', 4486)\n",
            "('HAD', 1)\n",
            "('always', 3697)\n",
            "('thought', 4418)\n",
            "('Jack', 1947)\n",
            "('Gisburn', 2831)\n",
            "('rather', 4244)\n",
            "('a', 4475)\n",
            "('cheap', 8)\n",
            "('genius', 9)\n",
            "('--', 4508)\n",
            "('though', 157)\n",
            "('good', 303)\n",
            "('fellow', 2418)\n",
            "('enough', 4458)\n",
            "('so', 4333)\n",
            "('it', 4496)\n",
            "('was', 4321)\n",
            "('no', 4511)\n",
            "('great', 4114)\n",
            "('surprise', 2650)\n",
            "('to', 4459)\n",
            "('me', 4498)\n",
            "('hear', 4163)\n",
            "('that', 4485)\n",
            "(',', 4504)\n",
            "('in', 4425)\n",
            "('the', 4480)\n",
            "('height', 73)\n",
            "('of', 4515)\n",
            "('his', 4469)\n",
            "('glory', 3325)\n",
            "('he', 4439)\n",
            "('had', 3932)\n",
            "('dropped', 2862)\n",
            "('painting', 4491)\n",
            "('married', 519)\n",
            "('rich', 787)\n",
            "('widow', 45)\n",
            "('and', 4505)\n",
            "('established', 48)\n",
            "('himself', 4402)\n",
            "('villa', 52)\n",
            "('on', 4471)\n",
            "('Riviera', 603)\n",
            "('.', 4517)\n",
            "('(', 768)\n",
            "('Though', 58)\n",
            "('would', 4403)\n",
            "('have', 4404)\n",
            "('been', 4412)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer Example"
      ],
      "metadata": {
        "id": "xXwO9k3qlH-v"
      },
      "id": "xXwO9k3qlH-v"
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleTokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;\"?_!()]|--|\\s)',text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'([,.:;\"?_!()]|--|\\s)',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "XJBOVSD0lKu1"
      },
      "id": "XJBOVSD0lKu1",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SampleTokenizerV1(vocab)\n",
        "text =  ''' I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
        "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
        "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
        "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
        "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.\n",
        "The desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.\n",
        "I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the \"deadening atmosphere of mediocrity\" (I quote Miss Croft) was having on him.\n",
        "'''\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf6WXu43moH2",
        "outputId": "4d81119b-e64d-41d4-92a8-3f097c739570"
      },
      "id": "Wf6WXu43moH2",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62, 52, 167, 1024, 70, 44, 840, 133, 274, 503, 13, 1023, 133, 517, 452, 409, 13, 929, 606, 1097, 731, 525, 982, 1037, 685, 1037, 554, 1006, 12, 589, 1007, 557, 744, 568, 513, 12, 550, 531, 387, 568, 770, 12, 683, 133, 862, 1123, 12, 175, 414, 566, 589, 133, 1086, 749, 1007, 98, 14, 10, 116, 62, 840, 1024, 606, 1142, 547, 226, 99, 756, 40, 14, 11, 1, 110, 557, 744, 568, 513, 1, 13, 1006, 1097, 1110, 1007, 1134, 260, 606, 14, 62, 262, 554, 81, 14, 43, 117, 13, 568, 624, 31, 918, 13, 343, 568, 1063, 134, 14, 1, 87, 315, 607, 515, 1037, 886, 1007, 1080, 744, 719, 791, 9, 1072, 16, 257, 62, 376, 1019, 744, 1006, 12, 80, 14, 97, 13, 1007, 668, 1037, 22, 605, 163, 62, 1019, 744, 14, 1, 110, 1138, 12, 749, 81, 14, 118, 656, 12, 715, 608, 132, 868, 132, 195, 1023, 1014, 1109, 848, 589, 174, 406, 1089, 744, 699, 14, 21, 606, 1097, 733, 753, 1007, 81, 14, 119, 1118, 710, 14, 53, 733, 1007, 427, 58, 34, 12, 198, 1007, 624, 47, 42, 903, 12, 950, 685, 227, 45, 1, 79, 1, 1037, 876, 12, 1130, 996, 589, 558, 434, 15, 1, 125, 895, 733, 664, 1074, 608, 651, 158, 1, 17, 126, 0, 13, 416, 1028, 1007, 817, 744, 59, 996, 62, 453, 135, 1037, 435, 1007, 437, 1130, 412, 14, 94, 70, 44, 0, 110, 1134, 531, 676, 565, 13, 606, 1097, 461, 1006, 1014, 900, 709, 565, 14, 20, 568, 764, 892, 455, 850, 1109, 555, 12, 175, 589, 568, 764, 1047, 545, 133, 716, 14, 95, 611, 17, 93, 14, 66, 606, 1109, 12, 1007, 572, 744, 1007, 317, 1097, 1087, 259, 658, 32, 86, 12, 1118, 12, 589, 163, 517, 442, 12, 252, 760, 589, 1007, 27, 133, 1085, 540, 1, 739, 1, 749, 70, 13, 751, 744, 1022, 905, 193, 948, 1130, 839, 997, 1006, 62, 547, 555, 10, 62, 1135, 876, 259, 1120, 11, 299, 1037, 45, 770, 14, 21, 929, 13, 568, 859, 231, 183, 604, 13, 1007, 362, 522, 354, 760, 12, 175, 12, 195, 81, 14, 117, 531, 808, 12, 1007, 814, 744, 1, 46, 1, 1108, 1072, 14, 68, 1097, 733, 1031, 1025, 1145, 626, 1006, 12, 589, 1007, 315, 744, 133, 454, 1106, 586, 749, 1007, 98, 12, 606, 971, 742, 1037, 685, 1037, 1136, 1121, 44, 531, 508, 1072, 568, 770, 14, 89, 849, 12, 606, 842, 1097, 133, 1000, 818, 14, 120, 143, 568, 1124, 1142, 547, 226, 1041, 392, 13, 568, 441, 919, 531, 226, 342, 1007, 930, 744, 877, 1006, 81, 14, 44, 531, 1, 381, 565, 379, 14, 1, 41, 81, 14, 44, 13, 195, 970, 13, 531, 733, 425, 1031, 724, 133, 1144, 156, 71, 859, 531, 226, 993, 14, 68, 693, 216, 1006, 550, 531, 683, 558, 13, 916, 550, 652, 568, 390, 13, 223, 550, 353, 1094, 1037, 514, 749, 770, 16, 257, 606, 1142, 547, 226, 544, 1037, 823, 1006, 550, 531, 508, 1072, 568, 770, 223, 550, 531, 683, 558, 14, 87, 315, 12, 587, 896, 531, 733, 381, 565, 379, 12, 896, 531, 411, 12, 195, 76, 34, 306, 12, 439, 1037, 1, 647, 565, 1072, 1, 13, 896, 531, 733, 638, 565, 209, 1037, 1007, 391, 14, 120, 828, 1007, 254, 600, 568, 538, 158, 13, 1110, 133, 1090, 473, 133, 1124, 0, 28, 81, 14, 44, 185, 1037, 547, 364, 606, 13, 175, 62, 453, 606, 693, 216, 599, 1037, 457, 760, 1121, 14, 110, 349, 645, 744, 1007, 98, 641, 609, 1037, 970, 826, 142, 937, 16, 175, 549, 12, 749, 719, 1103, 1037, 78, 30, 12, 268, 133, 511, 744, 71, 213, 1003, 236, 1007, 793, 12, 62, 531, 720, 242, 1021, 1007, 730, 332, 14, 62, 486, 1007, 314, 198, 995, 233, 1008, 773, 16, 175, 81, 14, 45, 1107, 1097, 929, 502, 1006, 12, 589, 1007, 410, 1105, 12, 62, 286, 606, 492, 14, 68, 1097, 733, 1006, 719, 574, 1097, 1, 599, 1, 15, 749, 1006, 801, 62, 310, 547, 508, 76, 34, 1007, 496, 845, 14, 68, 1097, 612, 223, 896, 1097, 132, 733, 132, 599, 13, 587, 62, 684, 216, 777, 1007, 255, 13, 1006, 62, 486, 558, 929, 14, 41, 70, 12, 163, 568, 645, 12, 531, 226, 984, 259, 599, 1134, 15, 1014, 531, 485, 568, 192, 12, 606, 531, 226, 843, 589, 1007, 575, 744, 1008, 151, 14, 21, 606, 1097, 1013, 598, 1037, 734, 1110, 395, 1007, 1, 335, 199, 744, 687, 1, 10, 62, 836, 76, 34, 11, 1097, 549, 749, 565, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding special context tokens"
      ],
      "metadata": {
        "id": "2l4tf4FRzMEw"
      },
      "id": "2l4tf4FRzMEw"
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znqx48SlzQRO",
        "outputId": "86da1ded-d20a-4c85-f4ab-a6e2aa34b6a6"
      },
      "id": "Znqx48SlzQRO",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE0Pak_Hzyn7",
        "outputId": "98031060-414a-4918-ecf3-76f0b78a601c"
      },
      "id": "SE0Pak_Hzyn7",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1151)\n",
            "('your', 1152)\n",
            "('yourself', 1153)\n",
            "('<|endoftext|>', 1154)\n",
            "('<|unk|>', 1155)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer V2"
      ],
      "metadata": {
        "id": "FzMeZvFm1DCR"
      },
      "id": "FzMeZvFm1DCR"
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleTokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;\"?_!()]|--|\\s)',text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int\n",
        "        else \"<|unk|>\" for item in preprocessed\n",
        "    ]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'([,.:;\"?_!()]|--|\\s)',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "wEI1HHt11FQM"
      },
      "id": "wEI1HHt11FQM",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SampleTokenizerV2(vocab)\n",
        "text1 =  ''' Hello Do you like tea, I love to play football'''\n",
        "text2 =  ''' Hello Do you like coffee, I love to play Soccer'''\n",
        "text  = \" <|endoftext|> \".join((text1,text2))\n",
        "print(text)\n",
        "print(\"\\n\")\n",
        "ids = tokenizer.encode(text)\n",
        "tokens = tokenizer.decode(ids)\n",
        "print(ids)\n",
        "print(\"\\n\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFLxfAOd2J5I",
        "outputId": "f2e9f414-f537-4e3d-f81c-e8ab89fa63cc"
      },
      "id": "FFLxfAOd2J5I",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello Do you like tea, I love to play football <|endoftext|>  Hello Do you like coffee, I love to play Soccer\n",
            "\n",
            "\n",
            "[1155, 1155, 1148, 651, 995, 12, 62, 1155, 1037, 1155, 1155, 1154, 1155, 1155, 1148, 651, 1155, 12, 62, 1155, 1037, 1155, 1155]\n",
            "\n",
            "\n",
            "<|unk|> <|unk|> you like tea , I <|unk|> to <|unk|> <|unk|> <|endoftext|> <|unk|> <|unk|> you like <|unk|> , I <|unk|> to <|unk|> <|unk|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ8PWYiqAFu-",
        "outputId": "39714397-2049-47d3-e5ce-7a9418483552"
      },
      "id": "SQ8PWYiqAFu-",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import importlib\n",
        "import tiktoken\n",
        "\n",
        "# print(importlib.metadata.version('tiktoken'))"
      ],
      "metadata": {
        "id": "x6TNjwJzAMpE"
      },
      "id": "x6TNjwJzAMpE",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is similar as the tokenizer we made above (SampleTokenizerV1/V2) but in one line now !\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "Sr9PODG8AhXL"
      },
      "id": "Sr9PODG8AhXL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"Hello do you like tea ? <|endoftext|> In the sunlit terraces\"\n",
        "\"of someunknownplaces\")\n",
        "print(text)\n",
        "print(\"\\n\")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFJDKA1bAsek",
        "outputId": "f41f094d-841f-46b7-c2ff-96523f1ef4d4"
      },
      "id": "kFJDKA1bAsek",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello do you like tea ? <|endoftext|> In the sunlit terracesof someunknownplaces\n",
            "\n",
            "\n",
            "[15496, 466, 345, 588, 8887, 5633, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 23625]\n",
            "\n",
            "\n",
            "Hello do you like tea ? <|endoftext|> In the sunlit terracesof someunknownplaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now using the orignal text and aplying BPE(Byte pair encoding)"
      ],
      "metadata": {
        "id": "wJ5wbVJBbYVk"
      },
      "id": "wJ5wbVJBbYVk"
    },
    {
      "cell_type": "code",
      "source": [
        "# import tiktoken\n",
        "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22GsHwsRbXx-",
        "outputId": "b3904847-8260-4d42-a166-e18635501147"
      },
      "id": "22GsHwsRbXx-",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "Te2R8-Csccyg"
      },
      "id": "Te2R8-Csccyg",
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}